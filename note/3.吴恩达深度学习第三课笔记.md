# 吴恩达深度学习第三课 学习笔记

> 二、网络正则化以及优化方法
> （1）视频教程：学习吴恩达deeplearning.ai系列教程中第二课和第三课所有内容。
> （2）学习目标：了解正则化方法、优化方法、数据集划分方式，学习超参数调节技巧。重点掌握mini-batch梯度下降法和batch norm正则化方法。
> （3）动手实验：完成第二课对应的课后编程作业并撰写报告，报告主要记录实验原理（mini-batch、batch norm理论推导等）、实验环境、实验结果、结果分析等。

**相关链接**
[吴恩达深度学习第三课](https://www.bilibili.com/video/BV1f4411C7Nx/)
[作业链接1](https://zhuanlan.zhihu.com/p/95510114)
[作业链接2](https://zhuanlan.zhihu.com/p/354386182)

## week-1

控制变量法调参(正交化方法)

使用单值评价指标

选择对不同样本平均错误率最小的模型

把某些性能作为优化指标（Optimizing metic），寻求最优化值；而某些性能作为满意指标（Satisficing metic），只要满足阈值就行了

通常，我们把training error与human-level error之间的差值称为偏差（bias），也称作avoidable bias；把dev error与training error之间的差值称为方差（variance）

学习算法的性能可以优于人类表现，但它永远不会优于贝叶斯错误的基准线。

**解决avoidable bias的常用方法**
Train bigger model
Train longer/better optimization algorithms: momentum, RMSprop, Adam
NN architecture/hyperparameters search

**解决variance的常用方法包括**
More data
Regularization: L2, dropout, data augmentation
NN architecture/hyperparameters search

## week-2

**迁移学习**
深度学习非常强大的一个功能之一就是有时候你可以将已经训练好的模型的一部分知识（网络结构）直接应用到另一个类似模型中去。  
训练好的网络的前面几层能提取到有用的特征

迁移学习的应用场合主要包括三点：Y
Task A and B have the same input x.
You have a lot more data for Task A than Task B.
Low level features from A could be helpful for learning B.

**多任务学习**
神经网络同时执行多个任务(一个图片中有多个分类物品)
应用场合
Training on a set of tasks that could benefit from having shared lower-level features.
Usually: Amount of data you have for each task is quite similar.
Can train a big enough neural network to do well on all the tasks.

**端到端学习**
将所有不同阶段的数据处理系统或学习系统模块组合在一起，用一个单一的神经网络模型来实现所有的功能。
