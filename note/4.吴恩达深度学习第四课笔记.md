# 吴恩达深度学习第四课 学习笔记

> 三、卷积神经网络（CNN）
> （1）视频教程：学习吴恩达deeplearning.ai系列教程中第四课前两周内容。
> （2）学习目标：学习卷积神经网络的基本结构，了解常见的卷积神经网络。重点掌握卷积和池化的计算以及CNN中的反向传播算法。
> （3）动手实验：完成第四课对应的课后编程作业并撰写报告，报告主要记录实验原理（CNN模型结构、反向传播理论推导等）、实验环境、实验结果、结果分析等。

**相关链接**
[吴恩达深度学习第四课](https://www.bilibili.com/video/BV1F4411y7o7/)
[作业链接1](https://zhuanlan.zhihu.com/p/95510114)
[作业链接2](https://zhuanlan.zhihu.com/p/354386182)

## week-1

### 1.1 卷积的引入

对于图片信息，如果将其展平，作为特征输入，有很多特征，训练效率低

### 1.2 & 1.3 卷积用于边缘检测

可以将卷积核作为w参数来训练

### 1.4 Padding

n\*n的矩阵与f\*f的卷积核卷积后，得到的矩阵大小为  
$$
(n-f+1) \times (n-f+1)
$$  
即矩阵会缩小，图像边界信息丢失

将n\*n的矩阵扩展(填充0)，四周扩展p像素(p: padding值)，得到(n+2p)\*(n+2p)的矩阵  
卷积后的矩阵大小为  
$$
(n+2p-f+1) \times (n+2p-f+1)
$$

valid: 不扩展进行卷积(p = 0)
same: 扩展后进行卷积，卷积后矩阵与原矩阵大小相同 ( $p = \frac{f - 1}{2}$ ) f常设为奇数

### 1.5 卷积步长 stride

s: 卷积步长stride  
p: padding  
n\*n的矩阵与f\*f的卷积核卷积后，得到的矩阵大小为
$$
\left \lfloor \frac{n + 2p - f}{s} + 1 \right \rfloor \times
\left \lfloor \frac{n + 2p - f}{s} + 1 \right \rfloor
$$
(卷积核超过原图像边界后不进行卷积，故向下取整)

### 1.6 三维卷积

对于rgb图像，有三层(height\*width\*channel)  
如果图像的channel和卷积核的一样，卷积结果为单层

例如:  
$(6 \times 6 \times 3)$ 的矩阵和 $(3 \times 3 \times 3)$ 的卷积核卷积
得到 $(4 \times 4)$ 的矩阵

如果使用多个卷积核进行卷积(得到不同方向的边界信息)
可以将图像与各个卷积核卷积的结果堆叠起来，得到多通道的结果

$$
(n \times n \times n_c) \otimes (f \times f \times n_c) =
((n - f + 1) \times (n - f + 1) \times {n_c}')
$$

${n_c}'$: 使用的卷积核数(卷积结果的通道数)(检测的特征数)

### 1.7 & 1.8 单层卷积神经网络

![1.7](https://github.com/lzsion/image-hosting/blob/master/DeepLearning/Snipaste_2023-10-13_11-10-43.png?raw=true)

<!-- ![1.7](imgs\Snipaste_2023-10-13_11-10-43.png) -->

这样，w参数即所有卷积核的大小，小于将图像展平后的参数

**卷积层的矩阵维度**
$f^{[l]}$ : 卷积核大小
$p^{[l]}$ : padding值
$s^{[l]}$ : 卷积步长stride
$n_{c}^{[l]}$ : 卷积核数量

输入矩阵大小为
$$
n_{H}^{[l-1]} \times n_{W}^{[l-1]} \times n_{c}^{[l-1]}
$$

卷积核(滤波器filter)大小为
$$
f^{[l]} \times f^{[l]} \times n_{c}^{[l-1]}
$$

则输出矩阵大小  
$$
n_{H}^{[l]} \times n_{W}^{[l]} \times n_{c}^{[l]} \\
n_{H}^{[l]} =
\left \lfloor
\frac{n_{H}^{[l-1]} + 2p^{[l]} - f^{[l]}}{s^{[l]}} + 1
\right \rfloor , \quad
n_{W}^{[l]} =
\left \lfloor
\frac{n_{W}^{[l-1]} + 2p^{[l]} - f^{[l]}}{s^{[l]}} + 1
\right \rfloor
$$

经过激活函数后( $a^{[l]}$ )大小
$$
n_{H}^{[l]} \times n_{W}^{[l]} \times n_{c}^{[l]}
$$
$A^{[l]}$ 大小
$$
m \times n_{H}^{[l]} \times n_{W}^{[l]} \times n_{c}^{[l]}
$$

权重参数W矩阵大小
$$
f^{[l]} \times f^{[l]} \times n_{c}^{[l-1]} \times n_{c}^{[l]}
$$

### 1.9 池化层

卷积神经网络中层的类型

- 卷积层 CONV (convolution)
- 池化层 POOL (pooling)
- 全连接层 FC (fully connected)

池化中**没有参数需要学习**  

**最大池化**
滤波器为取最大值  
滤波器的f，卷积的步长s，也是超参数  
最大池化只在一层(一个通道)上进行，每层(每通道)单独计算，最大池化后层数(通道数)不变  
池化后，高度h和宽度w会减小

**平均池化**
滤波器为取平均值  
不常用，在很深的网络可能会用  

**超参数**
滤波器的f  
卷积的步长s  
选择最大池化还是平均池化  
(padding值)很少用 (p=0)  

输入矩阵大小为
$$
n_{H} \times n_{W} \times n_{c}
$$
池化后输出矩阵大小为
$$
\left \lfloor \frac{n_{H} - f}{s} + 1 \right \rfloor \times
\left \lfloor \frac{n_{W} - f}{s} + 1 \right \rfloor \times
n_{c}
$$

通常卷积层后跟着池化层，这两层合在一起算作神经网络的一层(只计算有权重的层)

### 1.10 全连接层

深层节点的a展平后，进行标准的神经网络计算  
即标准的神经网络层，仅对特征向量计算  

随着网络深度加深， $n_H$ 和 $n_W$ 会减少，但通道数 $n_c$ 会增加，a的大小会减小  
随后进入全连接层

对于卷积神经网络，卷积层的参数较少，全连接层的参数较多

### 1.11 使用卷积神经网络的原因

**参数共享**  (卷积神经网络参数少的原因)  
即如果卷积核检测适用于一个区域，对其他区域也适用

**连接稀疏**  (卷积神经网络参数少的原因)  
z的值仅与卷积核大小范围的上一层的值有关  
即卷积核大小决定了z需要参考上一层的多少个值，卷积核size较小
故上下两层连接稀疏，防止了过拟合

**卷积神经网络善于抓住平移不变性**
卷积核在每次卷积的过程中都相当于移动了

## week-2

### 2.1 神经网络示例

经典神经网络

- LeNet-5
- AlexNet
- VGG

ResNet 残差网络

inception

### 2.2 经典神经网络
