{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 吴恩达深度学习第一课 作业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## week-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 用numpy构建sigmoid函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid函数\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1+e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.95257413])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 构建sigmoid的导数(梯度)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\sigma}'(z) = \n",
    "\\frac{-e^{-z}}{(1+e^{-z})^{2}} = \n",
    "\\frac{1}{1 + e^{-z}}(1 - \\frac{1}{1 + e^{-z}}) = \n",
    "\\sigma(z)(1-\\sigma(z))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(x):\n",
    "    s = sigmoid(x)\n",
    "    ds = s * (1 - s)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19661193, 0.10499359, 0.04517666])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "sigmoid_grad(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 np中的 shape 和 reshape 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X.shape用于获取矩阵/向量X的shape(维度)  \n",
    "X.reshape(...) 用于将X重塑为其他尺寸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = [[[0.53162315 0.52859771]\n",
      "  [0.68759983 0.58952027]\n",
      "  [0.36429866 0.96568101]]\n",
      "\n",
      " [[0.30466324 0.78140938]\n",
      "  [0.97725038 0.27599292]\n",
      "  [0.30757616 0.40457955]]\n",
      "\n",
      " [[0.77707515 0.75517586]\n",
      "  [0.39268482 0.83819877]\n",
      "  [0.85628051 0.28430032]]]\n",
      "a.shape = (3, 3, 2)\n",
      "b = [[0.53162315]\n",
      " [0.52859771]\n",
      " [0.68759983]\n",
      " [0.58952027]\n",
      " [0.36429866]\n",
      " [0.96568101]\n",
      " [0.30466324]\n",
      " [0.78140938]\n",
      " [0.97725038]\n",
      " [0.27599292]\n",
      " [0.30757616]\n",
      " [0.40457955]\n",
      " [0.77707515]\n",
      " [0.75517586]\n",
      " [0.39268482]\n",
      " [0.83819877]\n",
      " [0.85628051]\n",
      " [0.28430032]]\n",
      "b.shape = (18, 1)\n"
     ]
    }
   ],
   "source": [
    "a = np.random.rand(3,3,2)\n",
    "# a = np.random.rand(18, 1)\n",
    "\n",
    "print(\"a = \" + str(a))\n",
    "\n",
    "print(\"a.shape = \" + str(a.shape))\n",
    "\n",
    "b = a.reshape(a.shape[0] * a.shape[1] * a.shape[2], 1)\n",
    "# b = a.reshape(3 ,3, 2)\n",
    "\n",
    "print(\"b = \" + str(b))\n",
    "\n",
    "print(\"b.shape = \" + str(b.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 行标准化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数据归一化后，梯度下降的收敛速度更快，效果更好\n",
    "\n",
    "$$\n",
    "x \\to \\frac{x}{\\left \\| x \\right \\| }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (2, 3)\n",
      "x = [[1 1 8]\n",
      " [6 1 3]]\n",
      "x_norm.shape = (2, 1)\n",
      "x_norm = [[8.1240384 ]\n",
      " [6.78232998]]\n",
      "xx.shape = (2, 3)\n",
      "xx = [[0.12309149 0.12309149 0.98473193]\n",
      " [0.88465174 0.14744196 0.44232587]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randint(low=0, high=9, size=(2, 3))\n",
    "print(\"x.shape = \" + str(x.shape))  # x.shape = (2, 3)\n",
    "print(\"x = \" + str(x))\n",
    "\n",
    "x_norm = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "# axis=0：沿着列的方向执行操作，对每一列的元素进行聚合操作。\n",
    "# axis=1：沿着行的方向执行操作，对每一行的元素进行聚合操作。\n",
    "print(\"x_norm.shape = \" + str(x_norm.shape))    # x_norm.shape = (2, 1)\n",
    "print(\"x_norm = \" + str(x_norm))\n",
    "\n",
    "xx = x / x_norm\n",
    "# x 和 x_norm 维度不一致\n",
    "# x 和 x_norm 行数一致 x.shape = (2, 3)  x_norm.shape = (2, 1)\n",
    "# np计算时自动将 x_norm 的列扩展补全(broadcast)\n",
    "print(\"xx.shape = \" + str(x.shape))  # xx.shape = (2, 3)\n",
    "print(\"xx = \" + str(xx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "封装成函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeRows(x):    \n",
    "    x_norm = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "    xx = x / x_norm\n",
    "    return xx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.multiply(A, B) 和 A * B (相当于matlab的 A .* B ) 即相同位置的相乘\n",
    "np.dot(A, B) 是矩阵的向量乘\n",
    "加减乘除同理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## week-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 重塑维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个对于一个图片训练集/测试集 train_x_set  \n",
    "其shape为(m, px, py, 3)  \n",
    "重塑其维度(展平)  \n",
    "train_set_x_flatten = train_x_set.reshape(train_x_set.shape[0], -1).T  \n",
    "//将 train_x_set 重塑为 (m, px * py * 3) 然后转置 得到 (px * py * 3, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 标准化数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对图片 RGB值范围为[0, 255]  \n",
    "标准化只需  \n",
    "train = train_set_x_flatten / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 建立神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**步骤**  \n",
    "1. 定义模型结构(输入特征数量)  \n",
    "2. 初始化模型参数  \n",
    "3. 循环训练 正向传播 计算当前损失 & 反向传播 计算当前梯度 & 梯度下降 更新参数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义**传播函数**  \n",
    "propagate(w, b, X, Y)  \n",
    "实现前向和后向传播的传播函数，计算成本函数及其梯度。  \n",
    "\n",
    "参数：  \n",
    " - w  - 权重，大小不等的数组（num_px * num_px * 3，1）  \n",
    " - b  - 偏差，一个标量  \n",
    " - X  - 矩阵类型为（num_px * num_px * 3，训练数量）  \n",
    " - Y  - 真正的“标签”矢量（如果非猫则为0，如果是猫则为1），矩阵维度为(1,训练数据数量)  \n",
    "\n",
    "返回：  \n",
    " - cost - 逻辑回归的负对数似然成本  \n",
    " - dw  - 相对于w的损失梯度，因此与w相同的形状  \n",
    " - db  - 相对于b的损失梯度，因此与b的形状相同  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义**优化函数**  \n",
    "optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False)\n",
    "此函数通过运行梯度下降算法来优化w和b\n",
    "    \n",
    "参数：  \n",
    " - w  - 权重，大小不等的数组（num_px * num_px * 3，1）  \n",
    " - b  - 偏差，一个标量  \n",
    " - X  - 维度为（num_px * num_px * 3，训练数据的数量）的数组。  \n",
    " - Y  - 真正的“标签”矢量（如果非猫则为0，如果是猫则为1），矩阵维度为(1,训练数据的数量)  \n",
    " - num_iterations  - 优化循环的迭代次数  \n",
    " - learning_rate  - 梯度下降更新规则的学习率  \n",
    " - print_cost  - 每100步打印一次损失值  \n",
    "\n",
    "返回：  \n",
    " - params  - 包含权重w和偏差b的字典  \n",
    " - grads  - 包含权重和偏差相对于成本函数的梯度的字典  \n",
    " - 成本 - 优化期间计算的所有成本列表，将用于绘制学习曲线。  \n",
    "\n",
    "提示：  \n",
    "我们需要写下两个步骤并遍历它们：  \n",
    "1）计算当前参数的成本和梯度，使用propagate（）。  \n",
    "2）使用w和b的梯度下降法则更新参数。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义**预测函数**  \n",
    "predict(w, b, X)  \n",
    "使用学习逻辑回归参数 logistic(w，b) 预测标签是0还是1  \n",
    "\n",
    "参数：  \n",
    " - w  - 权重，大小不等的数组（num_px * num_px * 3，1）  \n",
    " - b  - 偏差，一个标量  \n",
    " - X  - 维度为（num_px * num_px * 3，训练数据的数量）的数据  \n",
    "\n",
    "返回：  \n",
    " - Y_prediction  - 包含X中所有图片的所有预测【0 | 1】的一个numpy数组（向量）  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义**模型函数**  \n",
    "model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False)  \n",
    "通过调用之前实现的函数来构建逻辑回归模型\n",
    "\n",
    "参数：\n",
    " - X_train  - numpy的数组,维度为（num_px * num_px * 3，m_train）的训练集\n",
    " - Y_train  - numpy的数组,维度为（1，m_train）（矢量）的训练标签集\n",
    " - X_test   - numpy的数组,维度为（num_px * num_px * 3，m_test）的测试集\n",
    " - Y_test   - numpy的数组,维度为（1，m_test）的（向量）的测试标签集\n",
    " - num_iterations  - 表示用于优化参数的迭代次数的超参数\n",
    " - learning_rate  - 表示optimize（）更新规则中使用的学习速率的超参数\n",
    " - print_cost  - 设置为true以每100次迭代打印成本\n",
    "\n",
    "返回：\n",
    " - d  - 包含有关模型信息的字典。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 绘制损失图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# costs = np.squeeze(d['costs'])\n",
    "# # 从字典 d 中提取出一个名为 'costs' 的项\n",
    "# # np.squeeze 移除数组中的单维度 确保 costs 是一个一维数组\n",
    "# plt.plot(costs)\n",
    "# plt.ylabel('cost')\n",
    "# plt.xlabel('iterations (per hundreds)')\n",
    "# plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 超参数的选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**迭代次数**  \n",
    "增加的迭代次数，训练集准确性提高了，但是测试集准确性却降低了。这称为过度拟合。  \n",
    "\n",
    "**学习率**\n",
    "学习率 $\\alpha$ 决定了我们更新参数的速度。    \n",
    "如果学习率过高，则成本可能会上下波动，甚至可能会发散，可能会“超过”最优值。\n",
    "如果它太小，我们将需要太多迭代才能收敛到最佳值，当训练精度比测试精度高很多时，就会发生过拟合情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## week-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 W和b参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        n_x - 输入层节点的数量\n",
    "        n_h - 隐藏层节点的数量\n",
    "        n_y - 输出层节点的数量    \n",
    "    返回：\n",
    "        parameters - 包含参数的字典：\n",
    "            W1 - 权重矩阵,维度为（n_h，n_x）\n",
    "            b1 - 偏向量，维度为（n_h，1）\n",
    "            W2 - 权重矩阵，维度为（n_y，n_h）\n",
    "            b2 - 偏向量，维度为（n_y，1）\n",
    "    \"\"\"    \n",
    "    \n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h) * 0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "\n",
    "    # 使用断言确保我的数据格式是正确的\n",
    "    assert(W1.shape == ( n_h , n_x ))\n",
    "    assert(b1.shape == ( n_h , 1 ))\n",
    "    assert(W2.shape == ( n_y , n_h ))\n",
    "    assert(b2.shape == ( n_y , 1 ))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 前向传播函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "         X - 维度为（n_x，m）的输入数据。\n",
    "         parameters - 初始化函数（initialize_parameters）的输出\n",
    "    \n",
    "    返回：\n",
    "         A2 - 使用sigmoid()函数计算的第二次激活后的数值\n",
    "         cache - 包含“Z1”，“A1”，“Z2”和“A2”的字典类型变量\n",
    "     \"\"\"\n",
    "    \n",
    "    # 从字典 “parameters” 中检索每个参数\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # 实现前向传播计算A2(概率)\n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    #使用断言确保我的数据格式是正确的\n",
    "    assert(A2.shape == (1,X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 计算cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large{[} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{]}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    计算方程中给出的交叉熵成本，\n",
    "    \n",
    "    参数：\n",
    "         A2 - 使用sigmoid()函数计算的第二次激活后的数值\n",
    "         Y - \"True\"标签向量,维度为（1，数量）         \n",
    "    \n",
    "    返回：\n",
    "         成本 - 交叉熵成本\n",
    "    \"\"\"\n",
    "    \n",
    "    # 样本数量\n",
    "    m = Y.shape[1] \n",
    "\n",
    "    # 计算交叉熵代价\n",
    "    logprobs = Y*np.log(A2) + (1-Y)* np.log(1-A2)\n",
    "    cost = -1/m * np.sum(logprobs)\n",
    "    \n",
    "    # 确保损失是我们期望的维度\n",
    "    # 例如，turns [[17]] into 17 \n",
    "    cost = np.squeeze(cost)     \n",
    "                               \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 反向传播函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    使用上述说明搭建反向传播函数。\n",
    "    \n",
    "    参数：\n",
    "     parameters - 包含我们的参数的一个字典类型的变量。\n",
    "     cache - 包含“Z1”，“A1”，“Z2”和“A2”的字典类型的变量。\n",
    "     X - 输入数据，维度为（2，数量）\n",
    "     Y - “True”标签，维度为（1，数量）\n",
    "    \n",
    "    返回：\n",
    "     grads - 包含W和b的导数一个字典类型的变量。\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # 首先，从字典“parameters”中检索W1和W2。\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "        \n",
    "    # 还可以从字典“cache”中检索A1和A2。\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    # 反向传播:计算 dW1、db1、dW2、db2。\n",
    "    dZ2= A2 - Y\n",
    "    dW2 = 1 / m * np.dot(dZ2,A1.T)\n",
    "    db2 = 1 / m * np.sum(dZ2,axis=1,keepdims=True)\n",
    "    dZ1 = np.dot(W2.T,dZ2) * (1-np.power(A1,2))\n",
    "    dW1 = 1 / m * np.dot(dZ1,X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1,axis=1,keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    使用上面给出的梯度下降更新规则更新参数\n",
    "    \n",
    "    参数：\n",
    "     parameters - 包含参数的字典类型的变量。\n",
    "     grads - 包含导数值的字典类型的变量。\n",
    "     learning_rate - 学习速率\n",
    "    \n",
    "    返回：\n",
    "     parameters - 包含更新参数的字典类型的变量。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 从字典“parameters”中检索每个参数\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # 从字典“梯度”中检索每个梯度\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    # 每个参数的更新规则\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 整合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        X - 数据集,维度为（2，示例数）\n",
    "        Y - 标签，维度为（1，示例数）\n",
    "        n_h - 隐藏层的数量\n",
    "        num_iterations - 梯度下降循环中的迭代次数\n",
    "        print_cost - 如果为True，则每1000次迭代打印一次成本数值\n",
    "    \n",
    "    返回：\n",
    "        parameters - 模型学习的参数，它们可以用来进行预测。\n",
    "     \"\"\"    \n",
    "    \n",
    "    # 初始化参数，然后检索 W1, b1, W2, b2。输入:“n_x, n_h, n_y”。\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    # 初始化参数，然后检索 W1, b1, W2, b2。\n",
    "    # 输入:“n_x, n_h, n_y”。输出=“W1, b1, W2, b2，参数”。\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # 循环(梯度下降)\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        # 前项传播\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # 计算成本\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    "        \n",
    "        # 反向传播\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        \n",
    "        # 更新参数\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "                  \n",
    "        # 每1000次迭代打印成本\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X): \n",
    "    \"\"\"\n",
    "    使用学习的参数，为X中的每个示例预测一个类\n",
    "    \n",
    "    参数：\n",
    "        parameters - 包含参数的字典类型的变量。\n",
    "        X - 输入数据（n_x，m）\n",
    "    \n",
    "    返回\n",
    "        predictions - 我们模型预测的向量（红色：0 /蓝色：1）\n",
    "     \n",
    "     \"\"\"\n",
    "    \n",
    "    # 使用前向传播计算概率，并使用 0.5 作为阈值将其分类为 0/1。\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = np.round(A2)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## week-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 搭建深层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与week-3类似"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
